# Week 2 torch.compile() Optimization Findings

**Date**: 2025-11-24
**Python Version**: 3.13.9
**PyTorch Version**: 2.5.1+cu121
**CUDA Device**: NVIDIA GeForce RTX 3080 Ti
**Assignee**: CUDA Optimization Engineer (Agent 4)

---

## Executive Summary

Week 2 quick wins strategy aimed to achieve **10-15x total speedup** through torch.compile() optimization (2-3x over existing 5x baseline). However, **critical compatibility issues prevent torch.compile() from working on Python 3.13**.

### Key Findings

1. **torch.compile() DOES NOT WORK on Python 3.13**
   - Error: "Dynamo is not supported on Python 3.13+"
   - Blocks all torch.compile() based optimizations
   - Impact: Lose 1.5-3x potential speedup

2. **FP16 autocast provides NO speedup** (0.93x - actually slower!)
   - Small molecules don't benefit from FP16
   - Overhead from dtype conversions exceeds compute savings
   - Only beneficial for large models with significant GEMM operations

3. **TorchScript JIT is SLOWER** (0.54x speedup - 1.85x slower!)
   - JIT overhead dominates for small, fast models
   - Graph tracing adds latency
   - Not suitable for this model architecture

### Critical Decision Required

**Option 1: Downgrade to Python 3.12** (Recommended)
- Enables torch.compile() (1.5-3x speedup expected)
- All PyTorch optimizations available
- Industry standard (most users on 3.11-3.12)
- Timeline: 1 day to migrate + 2 days to benchmark

**Option 2: Abandon torch.compile(), focus on batching + CUDA kernels**
- Fix batch processing bug (10-20x for batched workloads)
- Implement custom CUDA kernels (2-3x for single inference)
- Timeline: 1 week for batching, 2-3 weeks for CUDA
- Harder path, but achieves same goal

**Option 3: Hybrid - Python 3.12 + CUDA kernels**
- Best of both worlds
- 2-3x from torch.compile() + 2-3x from CUDA = **4-9x total**
- Timeline: 2-3 weeks total

---

## Detailed Benchmark Results

### Configuration Tested

| Configuration | torch.compile | FP16 | CUDA Graphs | JIT | Result |
|---------------|---------------|------|-------------|-----|--------|
| Baseline | No | No | No | No | ‚úì Works |
| torch.compile default | Yes | No | No | No | ‚ùå **FAILED** |
| torch.compile reduce-overhead | Yes | No | No | No | ‚ùå **FAILED** |
| torch.compile max-autotune | Yes | No | No | No | ‚ùå **FAILED** |
| torch.compile + CUDA graphs | Yes | No | Yes | No | ‚ùå **FAILED** |
| FP16 only | No | Yes | No | No | ‚ö†Ô∏è **SLOWER** (0.93x) |
| torch.compile + FP16 | Yes | Yes | No | No | ‚ùå **FAILED** |
| TorchScript JIT | No | No | No | Yes | ‚ö†Ô∏è **SLOWER** (0.54x) |
| JIT + FP16 | No | Yes | No | Yes | ‚ö†Ô∏è **SLOWER** (0.53x) |

**All torch.compile() configurations FAILED** with:
```
ERROR: Failed to create calculator: Dynamo is not supported on Python 3.13+
```

### Performance Comparison

**Baseline (No optimizations):**
- H2 (2 atoms): 0.300 ms
- H2O (3 atoms): 0.272 ms
- CH4 (5 atoms): 0.301 ms
- Benzene (12 atoms): 0.305 ms
- **Average: 0.294 ms**

**FP16 autocast:**
- H2: 0.366 ms (1.22x **slower**)
- H2O: 0.295 ms (1.08x **slower**)
- CH4: 0.311 ms (1.03x **slower**)
- Benzene: 0.299 ms (0.98x faster)
- **Average: 0.318 ms (0.93x overall = 7% SLOWER)**

**TorchScript JIT:**
- H2: 0.339 ms (1.13x slower)
- H2O: 0.901 ms (3.31x **SLOWER**)
- CH4: 0.467 ms (1.55x slower)
- Benzene: 0.490 ms (1.61x slower)
- **Average: 0.549 ms (0.54x = 1.87x SLOWER)**

---

## Root Cause Analysis

### Why torch.compile() Fails

**PyTorch Dynamo Limitation:**
- torch.compile() relies on TorchDynamo (torch._dynamo)
- Dynamo uses Python 3.11/3.12 specific features
- Python 3.13 changed internal APIs that Dynamo depends on
- PyTorch team has not yet added Python 3.13 support

**Timeline for Support:**
- PyTorch 2.6+ may add Python 3.13 support (ETA: Q1-Q2 2026)
- Currently no official timeline from PyTorch team

**Workaround:**
- Use Python 3.12 (latest supported version)
- Or wait for PyTorch 2.6+

### Why FP16 is Slower

**Small Model, Small Compute:**
- Model: 427K parameters (tiny!)
- Inference time: ~0.3 ms (already very fast)
- Bottleneck: NOT compute-bound

**FP16 Overhead:**
- Dtype conversions (FP32 ‚Üí FP16 ‚Üí FP32): ~0.02-0.05 ms
- Autocast context manager overhead: ~0.01 ms
- Reduced memory bandwidth savings: Negligible for small model
- **Total overhead > compute savings**

**When FP16 Helps:**
- Large models (>10M parameters)
- Compute-bound workloads (large GEMM operations)
- Batch size > 16
- Long sequences (transformers)

**Our case:**
- Small model, memory-bound, small batch ‚Üí **FP16 hurts**

### Why TorchScript JIT is Slower

**JIT Overhead:**
- Graph tracing: Extra overhead per forward pass
- Type checking: Dynamic typing overhead
- Optimization passes: Take time, minimal benefit for small graphs
- **Total overhead > optimization benefit**

**When JIT Helps:**
- Large computation graphs
- Complex control flow
- Batch size > 32
- Production serving (amortized over many calls)

**Our case:**
- Small, simple graph ‚Üí **JIT hurts**

---

## Revised Week 2 Strategy

Given torch.compile() doesn't work on Python 3.13, we have **3 options**:

### Option 1: Python 3.12 Migration (Quick, Low Risk)

**Timeline: 3 days**

**Day 1: Environment Setup**
- Create Python 3.12 conda environment
- Install dependencies (PyTorch, ASE, etc.)
- Verify all tests pass
- Run baseline benchmarks

**Day 2: torch.compile() Optimization**
- Test all compilation modes:
  - `mode='default'` (baseline)
  - `mode='reduce-overhead'` (latency-optimized)
  - `mode='max-autotune'` (throughput-optimized)
- Test CUDA graphs integration
- Test with FP16 (may work better with torch.compile())

**Day 3: Validation & Documentation**
- Run comprehensive benchmarks
- Validate accuracy (energy/forces)
- Create performance report
- Update deployment docs

**Expected Speedup:**
- torch.compile() alone: **1.5-2.5x**
- torch.compile() + FP16: **2-3x**
- torch.compile() + CUDA graphs: **2.5-3.5x**
- **Total: 2-3.5x over baseline**

**Risks:**
- Low (Python 3.12 is well-supported)
- May need to update some dependencies

**Recommendation:** ‚úÖ **RECOMMENDED - Fastest path to success**

---

### Option 2: Batch Processing + CUDA Kernels (No Python Change)

**Timeline: 2-3 weeks**

**Week 1: Fix Batch Processing**
- Debug current batch bug (28x slower than expected)
- Implement proper tensor batching
- Add padding/masking for variable sizes
- Benchmark batch sizes 1, 4, 8, 16, 32

**Week 2-3: Custom CUDA Kernels**
- Profile bottlenecks
- Implement fused kernels:
  - RBF computation + edge features
  - Message passing aggregation
  - Force scatter operations
- Optimize memory access patterns

**Expected Speedup:**
- Batch processing (fixed): **10-20x for batched workloads**
- Custom CUDA kernels: **1.5-2.5x for single inference**
- **Total: 1.5-2.5x single, 10-20x batch**

**Risks:**
- Medium-High (CUDA programming complexity)
- Longer timeline
- Harder to maintain

**Recommendation:** ‚ö†Ô∏è **FALLBACK - If Python 3.12 not possible**

---

### Option 3: Hybrid (Python 3.12 + CUDA + Batching)

**Timeline: 3-4 weeks**

**Week 1: Python 3.12 + torch.compile()**
- Migrate to Python 3.12
- Optimize torch.compile() settings
- Achieve 2-3x baseline speedup

**Week 2: Fix Batch Processing**
- Debug and fix batch bug
- Test batch efficiency

**Week 3-4: Custom CUDA Kernels**
- Profile post-compilation bottlenecks
- Implement targeted CUDA optimizations
- Focus on highest-impact kernels

**Expected Speedup:**
- torch.compile(): **2-3x**
- Batch processing: **10-20x for batches**
- CUDA kernels: **1.5-2x additional**
- **Total: 3-6x single, 30-60x batch**

**Risks:**
- Highest complexity
- Longest timeline
- Best performance

**Recommendation:** üéØ **STRETCH GOAL - Maximum performance**

---

## Immediate Recommendations

### 1. Decision Point: Python Version (TODAY)

**Question for User/Project Lead:**

> Should we migrate to Python 3.12 to unlock torch.compile() optimizations?

**If YES:**
- Start Python 3.12 migration immediately
- Expected 2-3x speedup in 3 days
- Aligns with Week 2 quick wins strategy

**If NO:**
- Abandon torch.compile() path
- Focus on batching + CUDA kernels
- Longer timeline (2-3 weeks)
- Similar final performance

### 2. Week 2 Deliverables (Adjusted)

**If Python 3.12:**
- Environment migration (Day 1)
- torch.compile() optimization (Day 2)
- Comprehensive benchmarks (Day 3)
- **Target: 2-3x speedup by end of week**

**If Python 3.13 (stay):**
- Debug batch processing (Days 1-2)
- Profile for CUDA opportunities (Day 3)
- **Target: Batch fix only (10-20x for batches)**

### 3. Week 3-4 Plan

**After Week 2 quick wins:**
- Implement custom CUDA kernels (highest impact)
- Optimize neighbor search (radius_graph)
- Fuse operations where possible
- **Target: 5-10x total speedup**

---

## Benchmark Data

Full benchmark results saved to:
```
/home/aaron/ATX/software/MLFF_Distiller/benchmarks/week2_compile_modes.json
```

### Summary Statistics

```json
{
  "baseline": {
    "mean_across_molecules_ms": 0.294,
    "configurations": ["H2", "H2O", "CH4", "Benzene"]
  },
  "fp16_only": {
    "mean_across_molecules_ms": 0.318,
    "speedup": 0.93
  },
  "jit": {
    "mean_across_molecules_ms": 0.549,
    "speedup": 0.54
  },
  "jit_fp16": {
    "mean_across_molecules_ms": 0.556,
    "speedup": 0.53
  }
}
```

**All torch.compile() configurations:** ‚ùå FAILED (Python 3.13 incompatibility)

---

## Alternative Optimizations (Python 3.13)

If staying on Python 3.13, focus on these optimizations:

### 1. Batch Processing (HIGHEST PRIORITY)

**Current Bug:** Batch-2 is 28x **slower** than single inference
**Expected:** Batch-16 should be ~10x **faster** per structure
**Impact:** **10-20x speedup for batched workloads**

### 2. Custom CUDA Kernels

**Target Operations:**
- Neighbor search (radius_graph): 1.5-2x
- RBF computation: 1.2-1.5x
- Message passing aggregation: 1.5-2x
- **Combined: 2-3x speedup**

### 3. Memory Optimizations

**Techniques:**
- Pre-allocate tensors
- Reuse buffers
- Pinned memory for CPU-GPU transfers
- **Combined: 1.2-1.5x speedup**

### 4. Triton Kernels (torch.compile() alternative)

**Approach:**
- Use Triton directly (doesn't require Dynamo)
- Write optimized kernels in Python-like syntax
- May work on Python 3.13
- **Expected: 1.5-2x speedup**

**Need to test:** Triton compatibility with Python 3.13

---

## Conclusion

Week 2 quick wins strategy **cannot be executed as planned** due to Python 3.13 limitations:

### What We Learned

1. ‚ùå torch.compile() **does NOT work** on Python 3.13
2. ‚ùå FP16 autocast **does NOT help** for small models
3. ‚ùå TorchScript JIT **makes things SLOWER**
4. ‚úÖ Baseline performance is already quite good (0.3 ms)
5. ‚úÖ Benchmark infrastructure is solid

### What We Need

**DECISION REQUIRED:** Python 3.12 migration vs. stay on 3.13

**Path A (Python 3.12):**
- ‚úÖ Quick (3 days)
- ‚úÖ Low risk
- ‚úÖ 2-3x speedup achievable
- ‚úÖ Enables all PyTorch optimizations

**Path B (Python 3.13):**
- ‚ö†Ô∏è Longer (2-3 weeks)
- ‚ö†Ô∏è Higher risk (CUDA programming)
- ‚ö†Ô∏è Similar final speedup
- ‚ö†Ô∏è Harder to maintain

### Recommended Action

1. **Consult user** on Python version preference
2. **If allowed:** Migrate to Python 3.12 immediately
3. **Execute Week 2 strategy** with torch.compile()
4. **Then proceed** to Weeks 3-4 (analytical gradients + CUDA)

**Final Target:** 20-30x total speedup (achievable with all optimizations combined)

---

## Next Steps

**TODAY:**
1. Get approval for Python 3.12 migration (or not)
2. If yes: Create Python 3.12 environment
3. If no: Start debugging batch processing

**THIS WEEK:**
- Complete either Path A or Path B Day 1-3
- Deliver Week 2 report with results
- Plan Weeks 3-4 execution

**WEEKS 3-4:**
- Implement analytical gradients (if Path A)
- Implement CUDA kernels (both paths)
- Achieve 10-15x intermediate target
- Push toward 20-30x final target

---

## Contact

For questions or decisions:
- **User**: Needs to approve Python version change
- **Coordinator**: M1 Coordinator (planning/scheduling)
- **Technical**: CUDA Optimization Engineer (Agent 4)

**Status:** Awaiting decision on Python version migration

Last Updated: 2025-11-24 (Week 2 Day 1)
